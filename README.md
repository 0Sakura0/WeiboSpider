# 关于该程序
## 目的
- 抓取通过PC端抓取微博用户信息
- 测试微博反爬虫机制


## 目前已完成工作
- 微博搜索、微博信息及用户信息解析模块
- 可以持续稳定的采集微博和微博用户相关数据(请求频率为**1s/次**，阈值正在测试)

## TODO
- [x] 添加搜索接口，可以对某个指定话题进行搜索
- [x] 修复某些时候抓取失败的问题
- [ ] 可视化展示
- [ ] 采用布隆过滤器去重网页
- [ ] 测试单机单账号访问阈值
- [ ] 测试单机多账号访问效果
- [ ] 改成分布式爬虫

## 配置和使用
- 安装相关依赖```pip install -r requirements.txt```
- 打开[配置文件](./config/spider.yaml)修改数据库相关配置
- 打开[sql文件](./config/sql/spider.sql)查看并使用建表语句
- 入口文件 
 - [search.py](./search_run.py):微博搜索程序:通过搜索接口搜索相关话题微博
 - [user_crawler.py](./user_crawler.py):微博用户抓取程序，通过指定特定用户来进行增量抓取
 - [repost.py](./repost.py):微博扩散程序，根据指定微博id，查看它转发(扩散)的情况

## 其它知识点
- 关于微博模拟登陆:可以查看我写的一篇[博客]()，里面详细介绍了微博模拟登陆的方法，有一些爬虫基础的同学可以完全无压力看懂

## 本项目目前的一些数据可视化展示(使用的**d3.js**):
对[某条指定微博](http://weibo.com/1973665271/E6HiqDiCg?refer_flag=1001030103_&type=comment#_rnd1473216182746)进行分析

微博扩散情况

![微博扩散](./img/kuosan.png)

转发该微博的用户性别比例

![用户性别比例](./img/sex.png)

转发该微博的时间

![转发曲线](./img/reposttime.png)

转发该微博的地域分析

![转发地域](./img/diyu.png)
